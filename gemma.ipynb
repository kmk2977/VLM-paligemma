{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional, Tuple, List\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import math\n",
    "from siglip import SiglipVisionConfig, SiglipVisionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache():\n",
    "    def __init__(self) -> None:\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "    \n",
    "    def num_items(self) -> int:\n",
    "        if len(self.key_cache) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            #key_cache [batch_size, num_heads_kv, seq_len, head_dim]\n",
    "            return self.key_cache[0].shape[-2]\n",
    "    \n",
    "    def update(self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            #if we have not added anything to KV-Cache lests create it\n",
    "            self.key_cache.append(key_states)\n",
    "            self.value_cache.append(value_states)\n",
    "        else:\n",
    "            #concatenate new keys #[batch_size, num_heads_kv, seq_len, head_dim]\n",
    "            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_cache], dim=-2)\n",
    "        #return all the keys\n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaConfig():\n",
    "    def __init__(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads,\n",
    "                head_dim=256, max_position_embeddings=8192, rms_norm_eps=1e-5, rope_theta=10000.0, attention_bias=False,\n",
    "                attention_dropout=0.0,pad_token_id=None,**kwargs):\n",
    "                super().__init__()\n",
    "                self.vocab_size = vocab_size\n",
    "                self.max_position_embeddings = max_position_embeddings\n",
    "                self.hidden_size = hidden_size\n",
    "                self.intermediate_size = intermediate_size\n",
    "                self.num_hidden_layers = num_hidden_layers\n",
    "                self.num_attention_heads = num_attention_heads\n",
    "                self.head_dim = head_dim\n",
    "                self.num_key_value_heads = num_key_value_heads\n",
    "                self.rms_norm_eps = rms_norm_eps\n",
    "                self.rope_theta = rope_theta\n",
    "                self.attention_bias = attention_bias\n",
    "                self.attention_dropout = attention_dropout\n",
    "                self.pad_token_id = pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaliGemmaConfig():\n",
    "    def __init__(self,vision_config=None,text_config=None,ignore_index=-100,image_token_index=256000,vocab_size=257152,\n",
    "                projection_dim=2048,pad_token_id=None,**kwargs):\n",
    "                super().__init__()\n",
    "                self.ignore_index = ignore_index\n",
    "                self.image_token_index = image_token_index\n",
    "                self.vocab_size = vocab_size\n",
    "                self.projection_dim = projection_dim\n",
    "                self.hidden_size = hidden_size\n",
    "                self.vision_config = vision_config\n",
    "                self.is_encoder_decoder = False\n",
    "                self.pad_token_id = pad_token_id\n",
    "\n",
    "                self.vision_config = SiglipVisionConfig(**vision_config)\n",
    "                self.text_config = text_config\n",
    "\n",
    "                self.text_config = GemmaConfig(**text_config, pad_token_id=pad_token_id)\n",
    "                self.vocab_size = self.text_config.vocab_size\n",
    "\n",
    "                self.text_config.num_image_tokens = (self.vision_config.image_size // self.vision_config.patch_size) ** 2\n",
    "                self.vision_config.projection_dim = projection_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaRMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) # 1 / sqrt(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float())\n",
    "        #llama uses x.to(float16) * w \n",
    "        #gemma uses (x * w).to(float16)\n",
    "        output = output * (1.0 + self.weight.float())\n",
    "        return output.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #equivalent to\n",
    "        #y = self.gate_proj(x) #[batxh_size, seq_len, hidden_size] -> [batch_size, seq_len, intermediate_size]\n",
    "        #y = torch.gelu(y, approximate='tanh') #[batch_size, seq_len, intermediate_size]\n",
    "        #j = self.up_proj(x) #[batch_size, seq_len, hidden_size] -> [batch_size, seq_len, intermediate_size]\n",
    "        #z = y * j #[batch_size, seq_len, intermediate_size]\n",
    "        #z = self.down_proj(z) #[batch_szie, seq_len, intermediate_size] -> [batch_size, seq_len, hidden_size]\n",
    "        return self.down_proj(nn.functional.gelu(self.gate_proj(x), approximate=\"tanh\") * self.up_proj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaAttention(nn.Module):\n",
    "    def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        assert self.hidden_size % self.num_heads == 0\n",
    "\n",
    "        #no of heads = 8\n",
    "        #hidden_size = 1024\n",
    "        #head_dim = 1024 / 8 = 128\n",
    "        #wq: [1024, 8 * 128] = [1024, 1024]\n",
    "        #wk: [1024, 2 * 128] = [1024, 256]\n",
    "        #wv: [1024, 2 * 128] = [1024, 256]\n",
    "        #so for every 4 heads of the query we have 1 head for the key\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * se;f.head_dim, bias = config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias = config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_attention_heads * self.head_dim, bias = config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
    "\n",
    "        self.rotary_emb = GemmaRotaryEmbedding(\n",
    "            self.head_dim,\n",
    "            max_position_embeddings=self.max_position_embeddings,\n",
    "            base=self.rope_theta\n",
    "        )\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None, kv_cache: Optional[KVCache] = None, \n",
    "                **kwargs) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        bsz, q_len, _ = hidden_states.size() #[batch_size, seq-Len, hidden_size]\n",
    "        #[batch_size, seq_len, num_heads_q * head_dim]\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        #[batch_size, seq_len, num_heads_KV * head_dim]\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        #[batch_size, seq_len, num_heads_kv * head_dim]\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        #[batch_size, num_heads_q, seq_len, head_dim]\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        #[batch_size, num)heads_kv, seq_len, head_dim]\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        #[batch_size, num_heads_kv,seq_len, head_dim]\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        #[batc_size, seq_len, head_dim], [batch_size, seq_len, head_dim]\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=None)\n",
    "        #[batch_size, num_heads_q, seq_len, head_dim], [batch_size, num_heads_kv, seq_len, head_dim]\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            key_states, value_states = kv_cache.update(key_states, value_states, self.layer_idx)\n",
    "\n",
    "        #repeat the key and values to match the number of heads of the query\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups) \n",
    "\n",
    "        #perform the calculation as usual, Q * K^T / sqrt(head_dim) #[batch_size, num_heads_q, seq_len_q, seq_len_kv]\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3) / math.sqrt(self.head_dim))\n",
    "\n",
    "        assert attention_mask is not None\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        #apply the softmask\n",
    "        #[batch_size, num_heads_q, seq_len_q, seq_len_kv]\n",
    "        attn_weights = nn.funtional.softmask(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        #apply dropout\n",
    "        attn_weights = nn.funtional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        #multiply by [batch_size, num_heads_q, seq_len_q, seq_len_kv] * [batch_size, num_heads_kv, seq_len_kv, head_dim] -> [batch_sizem num_heads_q, seq_len_q, head_dim]\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output,size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"'attn_output' should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "        \n",
    "        #make sure the sequence length is the second dimension #[batchS_size, num_heads_q, seq_len_q, head_dim] -> [batch_size, seq_len_q, num_heads_q, head_dim]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        #concatenate all the heads #[batch-size, seq_len_q, num_heads_q, head_dim] -> [batch_size, seq_len_q, num_heads_q * head_dim]\n",
    "        attn_output = attn_output.view(bsz, q_len, -1)\n",
    "\n",
    "        #multiply by W_o [batch_size, seq_len_q, hidden_size]\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: GemmaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = GemmaAttention(config=config, layer_idx=layer_idx)\n",
    "\n",
    "        self.nlp = GemmaMLP(config)\n",
    "        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.posr_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.Tensor] = None,\n",
    "                kv_cache: Optional[KVCache] = None) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        residual = hidden_states\n",
    "        #[batch_size, seq_len, hidden_size]\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        #[batch_size, seq_len, hidden_size]\n",
    "        hidden_states, _, = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            kv_cache=kv_cache\n",
    "        )\n",
    "        #[batch_size, seq_len, hidden_size]\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        #[batch_size, seq_len, hidden_size]\n",
    "        residual = hidden_states\n",
    "        #[batch_size, seq_len, hidden_size]\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        #[batch_size, seq_len, hidden_size]\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        #[batch_size, seq_len, hidden_size]\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaModel(nn.Module):\n",
    "    def __init__(self, config:GemmaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def forward(self, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.Tensor] = None,\n",
    "                inputs_embeds: Optional[torch.Tensor] = None, kv_cache: Optional[KVCache] = None) -> torch.FloatTensor:\n",
    "                #[batch_size, seq_len, hidden_size]\n",
    "                hidden_states = inputs_embeds\n",
    "                #[batch_size, seq_len, hidden_size]\n",
    "                normalizer = torch.Tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n",
    "                hidden_states = hidden_states * normalizer\n",
    "\n",
    "                for decoder_layer in self.layers:\n",
    "                    #[batch_size, seq_len, hidden_size]\n",
    "                    hidden_states = decoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask=attention_mask,position_ids=position_ids,kv_cache=kv_cache\n",
    "                    )\n",
    "                #[batch_size, seq_len, hidden_size]\n",
    "                hidden_states = self.norm(hidden_states)\n",
    "\n",
    "                #[batch_size, seq_len, hidden_size]\n",
    "                return hidden_states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = GemmaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "    \n",
    "    def tie_weights(self):\n",
    "        self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, attention_mask: Optional[torch.Tensor] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                inputs_embeds: Optional[torch.Tensor] = None,\n",
    "                kv_cache: Optional[KVCache] = None) -> Tuple:\n",
    "        \n",
    "        #outputs: [batch_size, seq_len, hidden_size]\n",
    "        outputs = self.model(attention_mask=attention_mask,position_ids=position_ids,inputs_embeds=inputs_embeds,kv_cache=kv_cache)\n",
    "        hidden_states =outputs\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        return_data = {\n",
    "            \"logits\": logits\n",
    "        }\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            #return the updated cache\n",
    "            return_data[\"kv_cache\"] = kv_cache\n",
    "\n",
    "        return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaliGemmaMultiModalProjector(nn.Module):\n",
    "    def __init__(self, config: PaliGemmaConfig):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(config.vision_config.hidden_size, config.vision_config.projection_dim, bias=True)\n",
    "    \n",
    "    def forward(self, image_features):\n",
    "        #[batch_size, num_patches, embed_dim] -> [batch_size, num_patches, projection_dim]\n",
    "        hidden_states = self.linear(image_features)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaliGemmaForConditionGeneration(nn.Module):\n",
    "    def __init__(self, config: PaliGemmaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_tower = SiglipVisionModel(config.vision_config)\n",
    "        self.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        language_model = GemmaForCausalLM(config.text_config)\n",
    "        self.language_model = language_model\n",
    "\n",
    "        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n",
    "    \n",
    "    def tie_weights(self):\n",
    "        return self.language_model.tie_weights()\n",
    "\n",
    "    def _merge_input_ids_with_image_features(\n",
    "        self, image_features: torch.Tensor, inputs_embeds: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor, kv_cache: Optional[KVCache] = None):\n",
    "        _,_, embed_dim = image_features.shape\n",
    "        batch_size, sequence_length = input_ids.shape\n",
    "        dtype, device = inputs_embeds.dtype, inputs_embeds.device\n",
    "        #shape:[batch_size, seq_len, hidden_size]\n",
    "        scaled_image_features = image_features / (self.config.hidden_size**0.5)\n",
    "\n",
    "        #combine the embeddings of the image tokens, the text tokens and mask out all the padding tokens\n",
    "        final_embedding = torch.zeros(batch_size, sequence_length, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n",
    "        #shape[betch_size, seq_len] true for text tokens\n",
    "        text_mask = (input_ids != self.config.image_token_index) & (input_ids != self.pad_token_id)\n",
    "        #shape[batch_size, seq_len] true for image tokens\n",
    "        image_mask = input_ids == self.config.image_token_index\n",
    "        #shape:[batch_size, seq_len] true for padding tokens\n",
    "        pad_mask = input_ids ==self.pad_token_id\n",
    "\n",
    "        #we need to expand the masks to the embedding dimension otherwise we cant use them in torch\n",
    "        text_mask_expanded = text_mask.unsqueeze(-1).expand(-1,-1,embed_dim)\n",
    "        pad_mask_expanded = pad_mask.unsqueeze(-1).expand(-1,-1,embed_dim)\n",
    "        image_mask_expanded = image_mask.unsqueeze(-1).expand(-1,-1,embed_dim)\n",
    "\n",
    "        #add the text embeddigns #the torch.where is used to check a condiiton and on the basis of that return either the para1 or para2\n",
    "        final_embedding = torch.where(text_mask_expanded, inputs_embeds, final_embedding)\n",
    "        #insert image embeddings. we cant use torch,wehre because the sequence lenght of scaled_image_features is not sequence length of the final embedding\n",
    "        final_embedding = final_embedding.masked_scatter(image_mask_expanded , scaled_image_features)\n",
    "        #zero out padding tokens\n",
    "        final_embedding = torch.where(pad_mask_expanded, torch.zeros_like(final_embedding), final_embedding)\n",
    "\n",
    "        #create attn mask\n",
    "        stype, device = inputs_embeds.dtype, inputs_embeds.device\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        q_len = inputs_embeds.shape[1]\n",
    "\n",
    "        if kv_cache is None or kv_cache.num_items() == 0:\n",
    "            #donot mask token as wer in prefill phase [only works when no padding]\n",
    "            causal_mask = torch.full(\n",
    "                (batch_size, q_len, q_len), fill_value=0, dtype=dtype, device=device\n",
    "            )\n",
    "        else:\n",
    "            #since we are generating tokens, query must be one single token\n",
    "            assert q_len == 1\n",
    "            kv_len = kv_cache.num_items() + q_len\n",
    "            #no need to mask, since each query should be able to attend all previous tokens\n",
    "            causal_mask = torch.full(\n",
    "                (batch_size, q_len, q_len), fill_value=0, dtype=dtype, device=device\n",
    "            )\n",
    "        \n",
    "        #add head dimension\n",
    "        #[batch_size, q_len, kv_len] -> [batch_size, num_heads_q, q_len, kv_len]\n",
    "        causal_mask = causal_mask.unsqueeze(1)\n",
    "        \n",
    "\n",
    "        if kv_cache is not None and kv_cache.num_items() > 0:\n",
    "            #the psoition of the query is just the last position\n",
    "            position_ids = attention_mask.cumsum(-1)[:, -1]\n",
    "            if position_ids.dim() == 1:\n",
    "                position_ids = position_ids.unsqueeze(0)\n",
    "        else:\n",
    "            #create a position_ids based on the size of the attention_mask\n",
    "            #for masked tokens, use the number 1 as position\n",
    "            position_ids = (attention_mask.cumsum(-1)).masker_fill_((attention_mask == 0), 1).to(device)\n",
    "        \n",
    "        return final_embedding, causal_mask, position_ids\n",
    "\n",
    "\n",
    "    def foward(self, input_ids: torch.LongTensor = None, Pixel_values: torch.FloatTensor = None,\n",
    "               attention_mask: Optional[torch.Tensor] = None, kv_cache: Optional[KVCache] = None) -> Tuple:\n",
    "               assert torch.all(attention_mask == 1), \"The input cannot be padded\"\n",
    "\n",
    "               #1. extract the input embeddings\n",
    "               #shape: (batch_size, seq_len, hidden_size)\n",
    "               inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n",
    "\n",
    "               #2. merge text and images\n",
    "               #[batch_size, channels, height, width] -> [batch_size, num_patches, embed_dim]\n",
    "               selected_image_feature = self.vision_tower(Pixel_values.to(inputs_embeds.dtype))\n",
    "               #[batch_size, num_patches, embed_dim] -> [batch_size, num_patches, hidden_size]\n",
    "               image_features = self.multi_modal_projector(selected_image_feature)\n",
    "\n",
    "               #merge the embeddings of the text tokens and image tokens\n",
    "               inputs_embeds, attention_mask, position_ids = self._merge_input_ids_with_image_features(image_features, inputs_embeds, input_ids, attention_mask, kv_cache)\n",
    "            \n",
    "               outputs = self.language_model(\n",
    "                attention_mask = attention_mask,\n",
    "                position_ids = position_ids,\n",
    "                inputs_embeds = inputs_embeds,\n",
    "                kv_cache = kv_cache\n",
    "               )\n",
    "\n",
    "               return outputs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
