{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional, Tuple, List\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import math\n",
    "from siglip import SiglipVisionConfig, SiglipVisionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaConfig():\n",
    "    def __init__(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads,\n",
    "                head_dim=256, max_position_embeddings=8192, rms_norm_eps=1e-5, rope_theta=10000.0, attention_bias=False,\n",
    "                attention_dropout=0.0,pad_token_id=None,**kwargs):\n",
    "                super().__init__()\n",
    "                self.vocab_size = vocab_size\n",
    "                self.max_position_embeddings = max_position_embeddings\n",
    "                self.hidden_size = hidden_size\n",
    "                self.intermediate_size = intermediate_size\n",
    "                self.num_hidden_layers = num_hidden_layers\n",
    "                self.num_attention_heads = num_attention_heads\n",
    "                self.head_dim = head_dim\n",
    "                self.num_key_value_heads = num_key_value_heads\n",
    "                self.rms_norm_eps = rms_norm_eps\n",
    "                self.rope_theta = rope_theta\n",
    "                self.attention_bias = attention_bias\n",
    "                self.attention_dropout = attention_dropout\n",
    "                self.pad_token_id = pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaliGemmaConfig():\n",
    "    def __init__(self,vision_config=None,text_config=None,ignore_index=-100,image_token_index=256000,vocab_size=257152,\n",
    "                projection_dim=2048,pad_token_id=None,**kwargs):\n",
    "                super().__init__()\n",
    "                self.ignore_index = ignore_index\n",
    "                self.image_token_index = image_token_index\n",
    "                self.vocab_size = vocab_size\n",
    "                self.projection_dim = projection_dim\n",
    "                self.hidden_size = hidden_size\n",
    "                self.vision_config = vision_config\n",
    "                self.is_encoder_decoder = False\n",
    "                self.pad_token_id = pad_token_id\n",
    "\n",
    "                self.vision_config = SiglipVisionConfig(**vision_config)\n",
    "                self.text_config = text_config\n",
    "\n",
    "                self.text_config = GemmaConfig(**text_config, pad_token_id=pad_token_id)\n",
    "                self.vocab_size = self.text_config.vocab_size\n",
    "\n",
    "                self.text_config.num_image_tokens = (self.vision_config.image_size // self.vision_config.patch_size) ** 2\n",
    "                self.vision_config.projection_dim = projection_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaliGemmaForConditionGeneration(nn.Module):\n",
    "    def __init__(self, config: PaliGemmaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_tower = SiglipVisionModel(config.vision_config)\n",
    "        self.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        language_model = GemmaForCausalLM(config.text_config)\n",
    "        self.language_model = language_model\n",
    "\n",
    "        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n",
    "    \n",
    "    def tie_weights(self):\n",
    "        return self.language_model.tie_weights()\n",
    "\n",
    "    def _merge_input_ids_with_image_features(\n",
    "        self, image_features: torch.Tensor, inputs_embeds: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor, kv_cache: Optional[KVCache] = None):\n",
    "        _,_, embed_dim = image_features.shape\n",
    "        batch_size, sequence_length = input_ids.shape\n",
    "        dtype, device = inputs_embeds.dtype, inputs_embeds.device\n",
    "        #shape:[batch_size, seq_len, hidden_size]\n",
    "        scaled_image_features = image_features / (self.config.hidden_size**0.5)\n",
    "\n",
    "        #combine the embeddings of the image tokens, the text tokens and mask out all the padding tokens\n",
    "        final_embedding = torch.zeros(batch_size, sequence_length, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n",
    "        #shape[betch_size, seq_len] true for text tokens\n",
    "        text_mask = (input_ids != self.config.image_token_index) & (input_ids != self.pad_token_id)\n",
    "        #shape[batch_size, seq_len] true for image tokens\n",
    "        image_mask = input_ids == self.config.image_token_index\n",
    "        #shape:[batch_size, seq_len] true for padding tokens\n",
    "        pad_mask = input_ids ==self.pad_token_id\n",
    "\n",
    "        #we need to expand the masks to the embedding dimension otherwise we cant use them in torch\n",
    "        text_mask_expanded = text_mask.unsqueeze(-1).expand(-1,-1,embed_dim)\n",
    "        pad_mask_expanded = pad_mask.unsqueeze(-1).expand(-1,-1,embed_dim)\n",
    "        image_mask_expanded = image_mask.unsqueeze(-1).expand(-1,-1,embed_dim)\n",
    "\n",
    "        #add the text embeddigns #the torch.where is used to check a condiiton and on the basis of that return either the para1 or para2\n",
    "        final_embedding = torch.where(text_mask_expanded, inputs_embeds, final_embedding)\n",
    "        #insert image embeddings. we cant use torch,wehre because the sequence lenght of scaled_image_features is not sequence length of the final embedding\n",
    "        final_embedding = final_embedding.masked_scatter(image_mask_expanded , scaled_image_features)\n",
    "        #zero out padding tokens\n",
    "        final_embedding = torch.where(pad_mask_expanded, torch.zeros_like(final_embedding), final_embedding)\n",
    "\n",
    "        #create attn mask\n",
    "        stype, device = inputs_embeds.dtype, inputs_embeds.device\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        q_len = inputs_embeds.shape[1]\n",
    "\n",
    "        if kv_cache is None or kv_cache.num_items() == 0:\n",
    "            #donot mask token as wer in prefill phase [only works when no padding]\n",
    "            causal_mask = torch.full(\n",
    "                (batch_size, q_len, q_len), fill_value=0, dtype=dtype, device=device\n",
    "            )\n",
    "        else:\n",
    "            #since we are generating tokens, query must be one single token\n",
    "            assert q_len == 1\n",
    "            kv_len = kv_cache.num_items() + q_len\n",
    "            #no need to mask, since each query should be able to attend all previous tokens\n",
    "            causal_mask = torch.full(\n",
    "                (batch_size, q_len, q_len), fill_value=0, dtype=dtype, device=device\n",
    "            )\n",
    "        \n",
    "        #add head dimension\n",
    "        #[batch_size, q_len, kv_len] -> [batch_size, num_heads_q, q_len, kv_len]\n",
    "        causal_mask = causal_mask.unsqueeze(1)\n",
    "        \n",
    "\n",
    "\n",
    "    def foward(self, input_ids: torch.LongTensor = None, Pixel_values: torch.FloatTensor = None,\n",
    "               attention_mask: Optional[torch.Tensor] = None, kv_cache: Optional[KVCache] = None) -> Tuple:\n",
    "               assert torch.all(attention_mask == 1), \"The input cannot be padded\"\n",
    "\n",
    "               #1. extract the input embeddings\n",
    "               #shape: (batch_size, seq_len, hidden_size)\n",
    "               inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n",
    "\n",
    "               #2. merge text and images\n",
    "               #[batch_size, channels, height, width] -> [batch_size, num_patches, embed_dim]\n",
    "               selected_image_feature = self.vision_tower(Pixel_values.to(inputs_embeds.dtype))\n",
    "               #[batch_size, num_patches, embed_dim] -> [batch_size, num_patches, hidden_size]\n",
    "               image_features = self.multi_modal_projector(selected_image_feature)\n",
    "\n",
    "               #merge the embeddings of the text tokens and image tokens\n",
    "               inputs_embeds, attention_mask, position_ids = self._merge_input_ids_with_image_features(image_features, inputs_embeds, input_ids, attention_mask, kv_cache)\n",
    "            \n",
    "               outputs = self.language_model(\n",
    "                attention_mask = attention_mask,\n",
    "                position_ids = position_ids,\n",
    "                inputs_embeds = inputs_embeds,\n",
    "                kv_cache = kv_cache\n",
    "               )\n",
    "\n",
    "               return outputs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
