{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import fire\n",
    "\n",
    "from processing_paligemma import PaliGemmaProcessor\n",
    "from gemma import KVCache, PaligemmaForConditionGeneration\n",
    "from utils import load_hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_inputs_to_device(model_inputs: dict, device: str):\n",
    "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inputs(\n",
    "    processor: PaliGemmaProcessor, prompt: str, image_file_path: str, device: str\n",
    "):\n",
    "    image = Image.open(image_file_path)\n",
    "    images = [image]\n",
    "    prompts = [prompt]\n",
    "    model_inputs = processor(text=prompts, images=images)\n",
    "    model_inputs = move_inputs_to_device(model_inputs, device)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(\n",
    "    model: PaliGemmaForConditionalGeneration,\n",
    "    processor: PaliGemmaProcessor,\n",
    "    device: str,\n",
    "    prompt: str,\n",
    "    image_file_path: str,\n",
    "    max_tokens_to_generate: int,\n",
    "    temperature: float,\n",
    "    top_p: float,\n",
    "    do_sample: bool,\n",
    "):\n",
    "    model_inputs = get_model_inputs(processor, prompt, image_file_path, device)\n",
    "    input_ids = model_inputs[\"input_ids\"]\n",
    "    attention_mask = model_inputs[\"attention_mask\"]\n",
    "    pixel_values = model_inputs[\"pixel_values\"]\n",
    "\n",
    "    kv_cache = KVCache()\n",
    "\n",
    "    # Generate tokens until you see the stop token\n",
    "    stop_token = processor.tokenizer.eos_token_id\n",
    "    generated_tokens = []\n",
    "\n",
    "    for _ in range(max_tokens_to_generate):\n",
    "        # Get the model outputs\n",
    "        # TODO: remove the labels\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            kv_cache=kv_cache,\n",
    "        )\n",
    "        kv_cache = outputs[\"kv_cache\"]\n",
    "        next_token_logits = outputs[\"logits\"][:, -1, :]\n",
    "        # Sample the next token\n",
    "        if do_sample:\n",
    "            # Apply temperature\n",
    "            next_token_logits = torch.softmax(next_token_logits / temperature, dim=-1)\n",
    "            next_token = _sample_top_p(next_token_logits, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        assert next_token.size() == (1, 1)\n",
    "        next_token = next_token.squeeze(0)  # Remove batch dimension\n",
    "        generated_tokens.append(next_token)\n",
    "        # Stop if the stop token has been generated\n",
    "        if next_token.item() == stop_token:\n",
    "            break\n",
    "        # Append the next token to the input\n",
    "        input_ids = next_token.unsqueeze(-1)\n",
    "        attention_mask = torch.cat(\n",
    "            [attention_mask, torch.ones((1, 1), device=input_ids.device)], dim=-1\n",
    "        )\n",
    "\n",
    "    generated_tokens = torch.cat(generated_tokens, dim=-1)\n",
    "    # Decode the generated tokens\n",
    "    decoded = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    print(prompt + decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_top_p(probs: torch.Tensor, p: float):\n",
    "    # (B, vocab_size)\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    # (B, vocab_size)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    # (B, vocab_size)\n",
    "    # (Substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    # Zero out all the probabilities of tokens that are not selected by the Top P\n",
    "    probs_sort[mask] = 0.0\n",
    "    # Redistribute the probabilities so that they sum up to 1.\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    # Sample a token (its index) from the top p distribution\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    # Get the token position in the vocabulary corresponding to the sampled index\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_path: str = None, prompt: str = None, image_file_path: str = None, max_tokens_to_generate: int = 100, temperature: float = 0.8,\n",
    "        top_p: float = 0.9, do_sample: bool = False, only_cpu: bool = False):\n",
    "        \n",
    "        device = \"cpu\"\n",
    "\n",
    "        if not only_cpu:\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda\"\n",
    "            elif torch.backends.mps.is_available():\n",
    "                device = \"mps\"\n",
    "        print(\"Device in use: \", device)\n",
    "\n",
    "        print(f\"Loading model\")\n",
    "        model, tokenizer = load_hf_model(model_path, device)\n",
    "        model = model.to(device).eval()\n",
    "\n",
    "        num_image_tokens = model.config.vision_config.num_image_tokens\n",
    "        image_size = model.config.vision_config.image_size\n",
    "        processor = PaliGemmaProcessor(tokenizer, num_image_tokens, image_size)\n",
    "\n",
    "        print(\"Running Inference\")\n",
    "        with torch.no_grad():\n",
    "            test_inference(model, processor, device, prompt, image_file_path, max_tokens_to_generate, temperature, top_p, do_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
