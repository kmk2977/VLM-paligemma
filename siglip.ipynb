{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionConfig:\n",
    "    def __init__(self, hidden_size=768,intermediate_size=3072,num_hidden_layers=12,num_attention_heads=12,num_channels=3,\n",
    "                    image_size=224,patch_size=16, layer_norm_eps=1e-6, attention_dropout=0.0, num_image_tokens: int = None, **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_channels = num_channels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.num_image_tokens = num_image_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=config.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding=\"valid\", #no padding\n",
    "        )\n",
    "\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_positions = self.num_patches\n",
    "        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.num_positions).expand((1,-1)),\n",
    "            persistent=False,\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        _, _, height, width = pixel_values.shape #[batch_size, channels, height, width]\n",
    "        #convolve the patch_size kernel over the image, with no overlapping patches since the stride is equal to kernel size\n",
    "        #the output of the convolution will have shape [batch_size, embed_dim, num_patches_h, num_patches_w]\n",
    "        #where num_patches_h = height // patch_size and num_patches_w = width // patch_size\n",
    "        patch_embeds = self.patch_embedding(pixel_values)\n",
    "        #[batch_size, embed_dim, num_patches_h, num_patches_w] -> [batch_size, embed_dim, num_patches]\n",
    "        #where num_patches = num_patches_h * num_patches_w\n",
    "        embeddings = patch_embeds.flatten(2)\n",
    "        #[batch_size, embed_dim, num_patches] -> [batch_size, num_patches, embed_dim]\n",
    "        embeddings = embeddings.transpose(1,2)\n",
    "        #add position embeddings to each patch. each positional encoding is a vector of size[embed_dim]\n",
    "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "        #[batch_size, num_patches, embed_dim]\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim**-0.5 #equivalent to 1 / sqrt(self.head_dim)\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        #hidden_states: [batch_size, num_patches, embed_dim]\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "        #query_states: [batch_size, num_patches, embed_dim]\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        #key_states: [batch_size, num_patches, embed_dim]\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        #value_states: [batch_size, num_patches, embed_dim]\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        #query_states: [batch_size, num_heads, num_patches, head_dim]\n",
    "        query_states = query_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(batch-size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        #calculate the attention using the formula Q * K^T / sqrt(d_k). attn_weigths: [batch_size, num_heads, num_patches, num_patches]\n",
    "        attn_weights = (torch.matmul(query_states, key_states.transpose(2,3)) * self.scale)\n",
    "\n",
    "        if attn_weights.size() != (batch_size, self.num_heads, seq_len, seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(batch_size, self.num_heads, seq_len, seq_len)}, but is f\" {attn_weights.size()}\"\"\n",
    "            )\n",
    "        \n",
    "        #apply the softmax row_wise . attn_weights: [batch_size, num_heads, num_patches, num_patches]\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        #apply dropout only during training\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        #multiply the attention weights with the value states attn_output: [batch_size, num_heads, num_patches, head_dim]\n",
    "        attn_output = torch.matmul(attn_weights, value_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipMLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        #[batch_size, num_patches, embed_dim] -> [batch_size, num_patches, intermediate_size]\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        #hidden_states: [batch_size, num_patches, intermediate_size]\n",
    "        hidden_states = nn.functional.gelu(hidden_states, approximate=\"tanh\")\n",
    "        #[batch_size, num_patches, intermediate_size] -> [batch_size, num_patches, embed_dim]\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = SiglipAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = SiglipMLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        # residual: [batch_size, num_patches, embed_dim] #aka skip connection\n",
    "        residual = hidden_states\n",
    "        #[batch_size, num_patches, embed_dim] -> [batch_size, num_patches, embed_dim]\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        #[batch_size, num_patches, embed_dim] -> [batch_size, num_patches, embed_dim]\n",
    "        hidden_states,_ = self.self_attn(hidden_states=hidden_states)\n",
    "        #[batch_size, num_patches, embed_dim]\n",
    "        hidden_states = residual + hidden_states\n",
    "        # residual: [batch_size, num_patches, embed_dim]\n",
    "        residual = hidden_states\n",
    "        #[batch_size, num_patches, embed_dim] -> [batch_size, num_patches, embed_dim]\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        #[batch_size, num_patches, embed_dim] -> [batch_size, num_patches, embed_dim]\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        #[batch_size, num_patches, embed_dim]\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = SiglipVisionEmbeddings(config)\n",
    "        self.encoder = SiglipEncoder(config)\n",
    "        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        # pixel_values: [Batch_size, channels, height, width] -> [Batch_size, Num_patches, Embed_dim]\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "        last_hidden_state = self.encoder(inputs_embeds=hidden_states)\n",
    "        last_hidden_state = self.post_layernorm(last_hidden_state)\n",
    "        return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_model = SiglipVisionTransformer(config)\n",
    "\n",
    "    def forward(self, pixel_values) -> Tuple:\n",
    "        #[Batch_size, channels, Height, Width] -> [Batch_size, Num_patches, Embed_dim]\n",
    "        return self.vision_model(pixel_values=pixel_values)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
